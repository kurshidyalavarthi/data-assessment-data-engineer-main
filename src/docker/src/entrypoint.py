import logging
from os import listdir
from os.path import isfile, join
import json
import sys
import pandas as pd

from vendor.pipeline import Pipeline
from db_generator.db_utils import generate_pony_orm_model

# initialize logging
logging.basicConfig(
    level = logging.INFO,
    format = "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers = [
        logging.FileHandler("container_execution.log"), logging.StreamHandler()
    ]
)

# Initialize the Database and Schema
logging.info('Generating Sqlite Database Schema')
generate_pony_orm_model()
# Import happens HERE because the functions being imported are generated by the above statement
sys.path.insert(0, '/code')
from data.gen.models import insert_Table_ats, select_statement

# Initialize 'python container tool'
pipeline = Pipeline()

#####################################################################################################
# NOTHING ABOVE THIS LINE NEEDS TO BE TOUCHED FOR THE ASSESSMENT (Although nobody is stopping you!) #
#####################################################################################################

# Pipeline steps
@pipeline.task()
def part_one__extract_raw_data(path: str):
    """
    Extracts the raw data from csv, xlsx, json files
    Args:
        path: location of raw data
    Returns: Pandas DataFrame that passes through the rest of the pipeline
    """
    logging.info('Extracting Raw Data...')

    logging.info('scanning input directory...')
    # scans and grabs only files
    onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]

    data = pd.DataFrame()
    #reading each file by format
    for file in onlyfiles:
        filepath = join(path, file)
        fileformat = file.split('.')[-1]
        if fileformat.lower() == 'csv':
            temp = pd.read_csv(filepath, index_col=False)
            temp.drop("Unnamed: 0", axis=1, inplace=True)

        elif fileformat.lower() == 'xlsx':

            temp = pd.read_excel(filepath, engine='openpyxl', index_col=False)
            temp.drop("Unnamed: 0", axis=1, inplace=True)

        elif fileformat.lower() == 'json':

            temp = pd.read_json(filepath)

        #Appending Parent Dataframe with current file data(temp)
        data = pd.concat([data, temp], ignore_index=True )
        #data = data.append(temp, ignore_index=True)   --- The frame.append method is deprecated and will be removed from pandas in a future version


    return data

@pipeline.task(depends_on=part_one__extract_raw_data)
def part_one__transform(dataset):
    """
    Following Transformations are done in the same order
        1. Common Status Mapping - striped, removed underscores and transformed to lower case on "application_status"
        2. Quarantine - removed records with invalid "phone" and "email"
        3. Normalization - applied on "phone"
    Args:
        dataset: pandas dateframe from extract
    Returns: list of dictionaries (acceptable by load function)
    """
    ## Common Status Mapping
    logging.info('Performing Transform 1: Common Status Mapping...')

    # makes application status into lower case, strips and replaces underscores("_")
    dataset["application_status"] = dataset["application_status"].str.lower().str.strip().replace('_', ' ', regex=True)



    ## Quarantine
    logging.info('Performing Transform 2: Quarantine...')

    #Casting "phone" datatype to string
    dataset['phone'] = dataset['phone'].astype(str)

    #extracting records where "phone" length is not either 10 or 11 digits
    phone_records_invalid = dataset[dataset["phone"].apply(lambda x: len(x) not in range(10, 12))]

    #droping the records that are captured above
    dataset.drop(phone_records_invalid.index, inplace=True)

    #extracting records where email format is not "string1@string2.string3"
    email_records_invalid = dataset[
        ~dataset["email"].str.contains("([A-Za-z0-9]+[.-_])*[A-Za-z0-9]+@[A-Za-z0-9-]+(\.[A-Z|a-z]{2,})+").astype(bool)]

    #appending all invalid records
    quarentine_records = pd.concat([phone_records_invalid,email_records_invalid], ignore_index=True)

    """
    Managing Quarentine options
    1. we can write the records to a remote location where business can access for further investigation
    2. Write to another table in the data lake/repository
    """

    # droping the records that are captured above for email records
    dataset.drop(email_records_invalid.index, inplace=True)



    ## Normalization
    logging.info('Performing Transform 3: Normalization...')

    # Normalizing phone number: matches a number comprised of either 10 digits, or 11 digits if the first number is 1
    dataset['phone'] = dataset["phone"].str.extract("^1?(\d{10})")


    return dataset.to_dict('records')


@pipeline.task(depends_on=part_one__transform)
def load_data(dataset):
    """
    Loads the supplied Data into the table,
    Nothing needs to be done here as part of the assessment

    Args:
        dataset: data from the last task having a format as follows (also depicted in line 44): [
            {
                'time': '2022-04-05 11:00:00.0',
                'person_name': 'Arthur Applicant',
                'phone': '2134567890',
                'email': 'arthur.applicant@cox.net',
                'company': 'Acme Anvil Corporation',
                'role': 'Wile E. Coyote Revivor',
                'application_status': 'disqualified'
            }
        ]
        Note: that each dictionary will represent a record and the keys of the dictionary MUST match the keys above in order to be added to the database.

    Returns: None
    """
    # Load Data Into Table
    logging.info('Loading Data in SQLite Database...')
    insert_Table_ats(dataset)

@pipeline.task(depends_on=load_data)
def part_two_query_one(input: None):
    """
    Great place to write the first query of part 2!
    Args:
        input: None
    Returns: None
    """

    logging.info('Running First Query...')
    # Select Statement - This Throws SQL over PonyORM (https://docs.ponyorm.org/queries.html)
    query = """
	company,
	application_status,
	printf("%.2f",(COUNT(*) / CAST(SUM(count(*)) OVER (PARTITION BY company) AS float )) * 100)  || "%"  as "percentage"
FROM
	ats
GROUP BY
	company,
	application_status
    """
    logging.info('Query:\n{}'.format(query))
    data = select_statement(query)
    logging.info('Results:\n{}'.format(
        json.dumps(data, indent = 2)
    ))

@pipeline.task(depends_on=part_two_query_one)
def part_two_query_two(input: None):
    """
    Great place to write the second query of part 2!
    Args:
        input: None
    Returns: None
    """

    logging.info('Running Second Query...')
    # Select Statement - This Throws SQL over PonyORM (https://docs.ponyorm.org/queries.html)
    query = """
	a.company,
	a.application_status as "application_status(prior to DQ)",
	printf ("%.2f",(COUNT(*) / CAST(SUM(count(*)) OVER (PARTITION BY a.company) AS float)) * 100) || "%" AS "percentage"
FROM
	ats a
	JOIN ( SELECT DISTINCT
			company,
			email
		FROM
			ats
		WHERE
			application_status = 'disqualified') b ON a.company = b.company
	AND a.email = b.email
WHERE
	a.application_status != 'disqualified'
GROUP BY
	a.company, a.application_status
    """
    logging.info('Query:\n{}'.format(query))
    data = select_statement(query)
    logging.info('Results:\n{}'.format(
        json.dumps(data, indent = 2)
    ))


#####################################################################################################
# NOTHING BELOW THIS LINE NEEDS TO BE TOUCHED FOR THE ASSESSMENT (Although nobody is stopping you!) #
#####################################################################################################

def process():
    data_loc = 'input'
    pipeline.run(f"{data_loc}")

if __name__ == '__main__':
    process()